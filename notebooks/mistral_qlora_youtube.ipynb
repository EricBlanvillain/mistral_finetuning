{"cells":[{"cell_type":"markdown","metadata":{"id":"eSjTSvTWv9Ji"},"source":["# Fine-tuning Mistral-7B-Instruct with QLoRA on YouTube Transcripts\n","\n","This notebook performs QLoRA fine-tuning on `mistralai/Mistral-7B-Instruct-v0.3` using a dataset derived from YouTube video transcripts.\n","\n","**Steps:**\n","1. Installs necessary libraries.\n","2. Sets up Hugging Face Hub authentication.\n","3. Loads and prepares the dataset (`train.jsonl`).\n","4. Configures the QLoRA parameters and loads the base model in 4-bit.\n","5. Sets up the `SFTTrainer` from the TRL library.\n","6. Runs the fine-tuning process.\n","7. Saves the trained LoRA adapter locally.\n","8. (Optional) Pushes the adapter to the Hugging Face Hub.\n","9. (Optional) Performs basic evaluation (Perplexity, ROUGE-L)."]},{"cell_type":"markdown","metadata":{"id":"jELQqsNFv9Jk"},"source":["## 1. Setup & Installs\n","\n","Install the required libraries. `bitsandbytes` requires a specific version compatible with Colab's GPU environment (usually T4 or A100)."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TZlSv_Gnwp5h","executionInfo":{"status":"ok","timestamp":1745192312326,"user_tz":-120,"elapsed":2270,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"5972a46b-0a67-40b4-a99a-af2e7d4ace24"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T6StSd3iv9Jl","executionInfo":{"status":"ok","timestamp":1745187751784,"user_tz":-120,"elapsed":91555,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"5d741cab-420c-48ce-c0e4-5e3ce25caf56"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/130.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.9/190.9 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m125.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.3/141.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.7/413.7 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m93.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.3/124.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\n","sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.38.2 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["# 1. Install core libraries\n","!pip install -q transformers==4.38.2 datasets==2.18.0 accelerate==0.27.2 peft==0.9.0 trl==0.7.11 torch torchvision torchaudio sentencepiece py7zr ninja huggingface_hub evaluate rouge_score pyyaml triton==3.2.0 bitsandbytes"]},{"cell_type":"markdown","metadata":{"id":"jZvRwgCjv9Jm"},"source":["## 2. Hugging Face Hub Authentication\n","\n","Log in to Hugging Face Hub to save the adapter and potentially download gated models. You'll need a User Access Token with `write` permissions.\n","\n","Get your token here: https://huggingface.co/settings/tokens"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17,"referenced_widgets":["f0eab43e78bc4e1bb11c132ed394ea4a","6c76c1cbe08e4eaa928b09d39f3c02fd","9ce120b5149a424eafe76ac9c36782ba","28e73934ba0b44aeaf3c38e56e67a224","057c4a9963c44da79289fcba1c903a76","7bf781c2027f463189959077a3f9ef0c","1ec8b066412048a0ad1173da5b02c954","a240cb7b880d484792c7940717392856","f14f3e7e81194f708ee1251351cd7111","8104f2a1b1d44ff39f312a40636b6053","554ed093c5bf4d79971c72066da71acf","fc8eec59c13449268a528a60c3453ccd","1a250eb351a44aca9003b46dbdf98267","07f640a537124fb1bfd1318a656dd9b8","f70c2b87e6d14b40932996b7a02e40af","9bcecae5240d452dbe2e298e8517fe0c","9d4c50b6589b46e684a819c2ee0bdfef","bc32f186451f4ecfbdd8e0460786144d","d705d1895ea14e849eff042d9ac7fa3b","1a98b211695947b2832bd7abfb5d397c"]},"id":"i8s5cq6Jv9Jm","executionInfo":{"status":"ok","timestamp":1745192314569,"user_tz":-120,"elapsed":358,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"2efd4dc3-78de-4955-e872-703634fef71f"},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0eab43e78bc4e1bb11c132ed394ea4a"}},"metadata":{}}],"source":["from huggingface_hub import login, notebook_login\n","# Use notebook_login() for interactive login in Colab/Jupyter\n","# or login(\"YOUR_HF_TOKEN\") if running in a script\n","notebook_login()"]},{"cell_type":"markdown","metadata":{"id":"XkxgbL8Tv9Jn"},"source":["## 3. Load and Prepare Dataset\n","\n","Upload your `train.jsonl` file (generated by `data_gen.py`) to your Colab session. You can do this using the file browser on the left panel.\n","\n","Alternatively, if you've pushed it to a Hugging Face dataset repository, you can load it directly from there."]},{"cell_type":"code","source":["cd /content/drive/MyDrive/mistral_finetuning"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vHemvQ7YwvI_","executionInfo":{"status":"ok","timestamp":1745192321079,"user_tz":-120,"elapsed":66,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"317f6017-f366-40a3-a340-36aa8669e139"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/mistral_finetuning\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qbHN-e37v9Jn","executionInfo":{"status":"ok","timestamp":1745192342516,"user_tz":-120,"elapsed":12092,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"3172b94f-7a1f-4e7d-f24b-e0dbc860941d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset from local files: train.jsonl, test.jsonl\n","Datasets loaded: Train size=2900, Eval size=724\n","Evaluation dataset loaded. Evaluation during training is enabled.\n","Dataset formatting function defined.\n","\n","Example formatted training sample:\n","[\"<s>[INST] According to the segment, what are the reasons and benefits of doing a mini cut in natural bodybuilding?\\n---\\nprobably super hungry um for the next six weeks you don't necessarily get more hungry or at least that's what I would infer based on the hormonal results from this study and it sort of does actually match my own personal experience with dieting now it is worth noting that in the study that I just referenced um the subjects started their cut very lean which is normal in natural bodybuilding uh and I think that their average was 99.6% body fat so it sort of Still Remains to be seen whether someone starting at say 12 to 15% would experience those same changes in Gin and leptin uh as someone who was starting leaner and then you could argue whether or not they would experience the same changes in hunger or not uh but those are sort of small differences in body fat and I think that 6 weeks should be more than enough time to increase your hunger if that's your goal so I still think that putting yourself in a sufficiently large deficit something like 20 to 25% below maintenance calories uh for 2 to 6 weeks should be enough to get your appetite going again and then you can get on with your sort of bulk or gaining mode or whatever all right so I think that the second and probably somewhat more common reason uh that you might want to mini cut um is just to keep your body fat down a little bit so um you know it seems to be the case at least from my observations that the guys at the top of the natural body building world right now tend to stay pretty lean in their offseason and they're extremely lean on show day and the standard for conditioning just continues to go up and up and so if you find yourself getting too sloppy in your offseason then this might either make life really difficult for you when you have to cut down again or it might just put a certain level of conditioning Out Of Reach for you for this season and so I think that mini cuts are a good way to sort of keep progressing in terms of adding muscle in the off season while keeping fat gain at Bay so here I'd like to break it down into two sort of separate examples that you probably fall somewhere on a\\n--- [/INST] Mini cuts are done to increase hunger by creating a large calorie deficit for 2 to 6 weeks and to keep body fat down during the offseason. This helps maintain a lean physique, making it easier to achieve better conditioning for show day and allows continued muscle gain while minimizing fat gain. </s>\"]\n"]}],"source":["import os\n","import torch\n","from datasets import load_dataset, DatasetDict # Import DatasetDict\n","from transformers import (\n","    AutoModelForCausalLM,\n","    AutoTokenizer,\n","    BitsAndBytesConfig,\n","    TrainingArguments,\n","    pipeline,\n","    logging,\n",")\n","from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n","from trl import SFTTrainer\n","import evaluate # For ROUGE score\n","import numpy as np\n","\n","# --- Configuration ---\n","# Model and Tokenizer\n","base_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","\n","# Dataset paths (ensure these files are uploaded to Colab)\n","train_dataset_path = \"train.jsonl\"\n","test_dataset_path = \"test.jsonl\" # Path to the test split\n","\n","# Option 2: Load from Hugging Face Hub (replace with your repo ID if you pushed the dataset)\n","# dataset_hub_id = \"your_username/your_dataset_repo_name\"\n","# dataset_files = {\"train\": \"train.jsonl\", \"test\": \"test.jsonl\"}\n","\n","# QLoRA config\n","bnb_config = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_quant_type=\"nf4\", # Recommended\n","    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for faster training\n","    bnb_4bit_use_double_quant=True, # Recommended\n",")\n","\n","# LoRA config\n","peft_config = LoraConfig(\n","    r=8,                 # LoRA attention dimension (rank)\n","    lora_alpha=16,       # Alpha parameter for scaling\n","    lora_dropout=0.1,   # Dropout probability for LoRA layers\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\",\n","    target_modules=[ # Find target modules using script below or common sense\n","        \"q_proj\",\n","        \"k_proj\",\n","        \"v_proj\",\n","        \"o_proj\",\n","        # \"gate_proj\", # Optional\n","        # \"up_proj\",   # Optional\n","        # \"down_proj\", # Optional\n","    ],\n",")\n","\n","# Training arguments\n","output_dir = \"./mistral-qlora-adapter_run4\" # Local directory to save adapter\n","per_device_train_batch_size = 2\n","gradient_accumulation_steps = 8\n","# num_train_epochs = 1.0 # Can use epochs or max_steps\n","max_steps = 175 # Adjust based on dataset size and desired training time (~200-400 recommended)\n","learning_rate = 1e-4\n","optim = \"paged_adamw_32bit\" # Recommended optimizer for QLoRA\n","logging_steps = 10\n","save_steps = 25 # Save checkpoints periodically\n","max_grad_norm = 0.3\n","warmup_ratio = 0.03\n","lr_scheduler_type = \"constant\" # Or \"cosine\", \"linear\"\n","evaluation_strategy = \"steps\" # Evaluate during training using the test set\n","eval_steps = 25             # Evaluate every N steps\n","# report_to=\"tensorboard\" # Or wandb\n","\n","# SFT Trainer specific\n","max_seq_length = MAX_CHUNK_TOKENS = 512 # Defined in data_gen.py, ensure consistency\n","packing = False # Set to True if you want to pack sequences, requires more memory\n","\n","# Hugging Face Hub repo ID (optional)\n","hf_hub_repo_id = \"your_username/mistral-7b-instruct-youtube-qlora\" # CHANGE THIS to your HF username/repo name\n","\n","# --- Load Dataset ---\n","train_dataset = None\n","eval_dataset = None\n","\n","try:\n","    # Check if local files exist\n","    if os.path.exists(train_dataset_path) and os.path.exists(test_dataset_path):\n","        print(f\"Loading dataset from local files: {train_dataset_path}, {test_dataset_path}\")\n","        # Load both files into a DatasetDict\n","        dataset = load_dataset('json', data_files={'train': train_dataset_path, 'test': test_dataset_path})\n","        train_dataset = dataset['train']\n","        eval_dataset = dataset['test'] # Use the 'test' split for evaluation\n","        print(f\"Datasets loaded: Train size={len(train_dataset)}, Eval size={len(eval_dataset)}\")\n","    # elif dataset_hub_id: # Option to load from Hub\n","    #     print(f\"Local files not found. Attempting to load from Hub: {dataset_hub_id}\")\n","    #     dataset = load_dataset(dataset_hub_id, data_files=dataset_files)\n","    #     train_dataset = dataset['train']\n","    #     eval_dataset = dataset['test']\n","    #     print(f\"Datasets loaded from Hub: Train size={len(train_dataset)}, Eval size={len(eval_dataset)}\")\n","    else:\n","        missing_files = []\n","        if not os.path.exists(train_dataset_path): missing_files.append(train_dataset_path)\n","        if not os.path.exists(test_dataset_path): missing_files.append(test_dataset_path)\n","        raise FileNotFoundError(f\"Dataset file(s) not found. Please upload: {', '.join(missing_files)}\")\n","\n","except Exception as e:\n","    print(f\"Error loading dataset: {e}\")\n","    # Stop execution if datasets aren't loaded\n","    # exit()\n","\n","# Ensure evaluation strategy is set correctly if eval_dataset exists\n","if eval_dataset is None:\n","    evaluation_strategy = \"no\"\n","    eval_steps = None\n","    print(\"Warning: No evaluation dataset loaded. Disabling evaluation during training.\")\n","else:\n","    # Keep evaluation_strategy and eval_steps as defined earlier\n","    print(\"Evaluation dataset loaded. Evaluation during training is enabled.\")\n","\n","\n","# --- Format dataset for SFTTrainer ---\n","# Mistral Instruct format:\n","# <s>[INST] Instruction [/INST] Answer </s>\n","# We need a function that takes a sample and returns a formatted string.\n","\n","def format_instruction(sample):\n","    # Uses the 'instruction', 'input', and 'output' fields from train.jsonl/test.jsonl\n","    # 'input' contains the original transcript chunk\n","    # 'output' contains the LLM-generated answer\n","    instruction = sample['instruction']\n","    context = sample['input'] # The transcript chunk\n","    response = sample['output'] # The LLM-generated answer\n","\n","    # Combine instruction and context for the prompt\n","    prompt = f\"{instruction}\\n---\\n{context}\\n---\" # Separators help delineate\n","\n","    # Format according to Mistral Instruct template\n","    return [f\"<s>[INST] {prompt} [/INST] {response} </s>\"]\n","\n","print(\"Dataset formatting function defined.\")\n","# Example of formatted text:\n","if train_dataset and len(train_dataset) > 0:\n","    print(\"\\nExample formatted training sample:\")\n","    print(format_instruction(train_dataset[0]))\n","else:\n","    print(\"Train dataset is empty or not loaded, cannot show example.\")"]},{"cell_type":"markdown","metadata":{"id":"CEkwgxMkv9Jo"},"source":["## 4. Load Model and Tokenizer with QLoRA Config\n","\n","Load the base model (`Mistral-7B-Instruct-v0.3`) with 4-bit quantization using the `BitsAndBytesConfig`. We also load the corresponding tokenizer."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120,"referenced_widgets":["96b1bb79d4cf4f91b4edab9ccfbb976b","e3fe2a8a5c35422196893ef8521daef2","a98a019d2d1d46ef80e590a69a903d94","48cbaf251f924c99a22321ae143adce0","2e81d12660a74bf1b7174e277b4c0b1b","258fb06e9dc34b0eb47aed1b783d2861","9ec020310c3d4f5ba05f453d13b2ce86","9152e89d5eee4c16b39327d584305679","810f48a49f384d968219efa7ba5d80b3","057e9950cd1946909832069f5691dc60","257575efec914a469283c0658e4ae333"]},"id":"cQB7-Hpiv9Jo","executionInfo":{"status":"ok","timestamp":1745188176539,"user_tz":-120,"elapsed":10981,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"6fb5bb5a-2d6f-4eca-bb27-31b93df3822d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenizer loaded.\n","Loading base model: mistralai/Mistral-7B-Instruct-v0.3 with 4-bit quantization...\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b1bb79d4cf4f91b4edab9ccfbb976b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Base model loaded.\n","Model prepared for training.\n"]}],"source":["# Load Tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n","tokenizer.pad_token = tokenizer.eos_token # Set pad token\n","tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n","print(\"Tokenizer loaded.\")\n","\n","# Load Model with QLoRA config\n","print(f\"Loading base model: {base_model_name} with 4-bit quantization...\")\n","model = AutoModelForCausalLM.from_pretrained(\n","    base_model_name,\n","    quantization_config=bnb_config,\n","    device_map=\"auto\", # Automatically map layers to GPU\n","    trust_remote_code=True, # Necessary for some models\n","    # torch_dtype=torch.bfloat16, # dtype is set in bnb_config\n",")\n","print(\"Base model loaded.\")\n","\n","# --- Sanity Check: Find LoRA Target Modules ---\n","# Uncomment the following lines to see all linear layer names\n","# This helps verify the `target_modules` in LoraConfig\n","# print(\"\\nModel Architecture:\")\n","# print(model)\n","# print(\"\\nFinding potential LoRA target modules (Linear layers):\")\n","# linear_layers = set()\n","# for name, module in model.named_modules():\n","#     if isinstance(module, torch.nn.Linear):\n","#          #Focus on layers typically targeted by LoRA in transformers\n","#          if any(layer_name in name for layer_name in ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']):\n","#              # Get the last part of the name (e.g., 'q_proj')\n","#              layer_name = name.split('.')[-1]\n","#              linear_layers.add(layer_name)\n","# print(f\"Found linear layer names: {linear_layers}\")\n","# print(f\"Using target modules: {peft_config.target_modules}\")\n","# print(\"Ensure these match the typical layers for Mistral architecture.\")\n","\n","# --- Prepare model for k-bit training ---\n","# Cast layer norms and head to fp32 for stability\n","# model = prepare_model_for_kbit_training(model) # TRL's SFTTrainer handles this\n","\n","# --- Create PEFT Model ---\n","# Note: SFTTrainer can also handle PEFT model creation if peft_config is passed\n","# Creating it explicitly here for clarity\n","# print(\"\\nApplying LoRA adapter to the base model...\")\n","# model = get_peft_model(model, peft_config)\n","# print(\"LoRA adapter applied.\")\n","# model.print_trainable_parameters()\n","\n","# Configure cache usage (optional, but recommended)\n","model.config.use_cache = False # Important for training stability with gradient checkpointing\n","# model.config.pretraining_tp = 1 # If you face tensor parallelism issues\n","print(\"Model prepared for training.\")"]},{"cell_type":"markdown","metadata":{"id":"3tj3KccRv9Jp"},"source":["## 5. Configure SFTTrainer\n","\n","We use the `SFTTrainer` from the TRL library, which simplifies the process of supervised fine-tuning for instruction-following tasks."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9J3vqIQ9v9Jp","executionInfo":{"status":"ok","timestamp":1745189122587,"user_tz":-120,"elapsed":250,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"8bd20c21-1eb6-4fb1-f73c-178db57f6c24"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training Arguments configured.\n","SFTTrainer initialized.\n","\n","BF16 is supported. Training will use BF16.\n","Trainer arguments updated based on hardware support.\n"]}],"source":["training_arguments = TrainingArguments(\n","        output_dir=output_dir,\n","        per_device_train_batch_size=per_device_train_batch_size,\n","        gradient_accumulation_steps=gradient_accumulation_steps,\n","        optim=optim,\n","        save_steps=save_steps,\n","        logging_steps=logging_steps,\n","        learning_rate=learning_rate,\n","        # num_train_epochs=num_train_epochs,\n","        max_steps=max_steps,\n","        fp16=False, # Use bf16 if available (Ampere GPUs like A100)\n","        bf16=True, # Set to True for Ampere GPUs, False for T4 (if bnb_compute_dtype is bfloat16)\n","        max_grad_norm=max_grad_norm,\n","        warmup_ratio=warmup_ratio,\n","        group_by_length=True, # Speeds up training by grouping similar length sequences\n","        lr_scheduler_type=lr_scheduler_type,\n","        # Evaluation settings (only if eval_dataset is provided)\n","        evaluation_strategy=evaluation_strategy, # Use evaluation_strategy for transformers 4.38.2\n","        eval_steps=eval_steps,\n","        report_to=\"none\",\n","        # Pushing to Hub options\n","        # push_to_hub=True, # Set to True to push model/adapter during training\n","        # hub_model_id=hf_hub_repo_id, # Repository name on Hugging Face Hub\n","        # hub_strategy=\"checkpoint\", # Push on every save\n","        # hub_token=os.getenv(\"HF_TOKEN\") # Use token stored in environment or login()\n","    )\n","\n","print(\"Training Arguments configured.\")\n","\n","trainer = SFTTrainer(\n","    model=model,\n","    train_dataset=train_dataset,\n","    eval_dataset=eval_dataset, # Pass evaluation dataset here\n","    peft_config=peft_config, # Pass PEFT config here\n","    # dataset_text_field=\"text\", # Use if you pre-formatted into a 'text' column\n","    formatting_func=format_instruction, # Pass the formatting function\n","    tokenizer=tokenizer,\n","    args=training_arguments,\n","    packing=packing,\n",")\n","\n","print(\"SFTTrainer initialized.\")\n","# TRL automatically handles prepare_model_for_kbit_training when peft_config is passed\n","# model.print_trainable_parameters()\n","\n","# Verify bf16 setting based on GPU availability\n","if torch.cuda.is_bf16_supported():\n","    print(\"\\nBF16 is supported. Training will use BF16.\")\n","    if not training_arguments.bf16:\n","      print(\"Warning: BF16 supported but not enabled in TrainingArguments. Enabling it.\")\n","      training_arguments.bf16 = True\n","      training_arguments.fp16 = False # Ensure fp16 is off if bf16 is on\n","else:\n","    print(\"\\nBF16 is NOT supported. Ensure compute_dtype in BitsAndBytesConfig is appropriate (e.g., float16) and bf16=False in TrainingArguments.\")\n","    if training_arguments.bf16:\n","        print(\"Warning: BF16 is not supported, but bf16=True in TrainingArguments. Setting bf16=False and fp16=True.\")\n","        training_arguments.bf16 = False\n","        training_arguments.fp16 = True # Fallback to fp16 if bf16 not available\n","\n","# Re-initialize trainer if arguments changed (e.g., bf16 status)\n","# This might not be strictly necessary as args are references, but safer\n","trainer.args = training_arguments\n","print(\"Trainer arguments updated based on hardware support.\")"]},{"cell_type":"markdown","metadata":{"id":"o8XNJ7yvv9Jp"},"source":["## 6. Start Fine-tuning\n","\n","Launch the training process. This will take some time depending on the dataset size, `max_steps`, and the Colab GPU assigned (T4 is slower than A100). Aiming for < 2 hours on a T4."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"7mPAxKnfv9Jp","executionInfo":{"status":"ok","timestamp":1745189336602,"user_tz":-120,"elapsed":208209,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"3943afa7-d33a-4a0f-e53a-28407b2f9d2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Starting fine-tuning...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='175' max='175' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [175/175 03:26, Epoch 175/175]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>25</td>\n","      <td>0.322000</td>\n","      <td>2.660602</td>\n","    </tr>\n","    <tr>\n","      <td>50</td>\n","      <td>0.011600</td>\n","      <td>3.871646</td>\n","    </tr>\n","    <tr>\n","      <td>75</td>\n","      <td>0.007400</td>\n","      <td>4.020560</td>\n","    </tr>\n","    <tr>\n","      <td>100</td>\n","      <td>0.005300</td>\n","      <td>3.682944</td>\n","    </tr>\n","    <tr>\n","      <td>125</td>\n","      <td>0.004700</td>\n","      <td>3.591010</td>\n","    </tr>\n","    <tr>\n","      <td>150</td>\n","      <td>0.003600</td>\n","      <td>3.584726</td>\n","    </tr>\n","    <tr>\n","      <td>175</td>\n","      <td>0.003200</td>\n","      <td>3.703533</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json (Request ID: Root=1-68057925-30eb8b2336f64bfe7c5962b7;3e242787-b934-4d9c-b0c5-65144c261347)\n","\n","Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json (Request ID: Root=1-68057943-6edafe7355cb26a039ffb698;03680d8d-bcdf-4ae4-a42c-a5060ecb9265)\n","\n","Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json (Request ID: Root=1-68057961-375f52cf4462e06c5b08acfc;76f71c77-d719-4b8b-ba89-e2e7fd807647)\n","\n","Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json (Request ID: Root=1-6805797e-23b5c4a43bd836a13db17ab9;3c18dba4-8164-460f-888c-3181e2b9856d)\n","\n","Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json (Request ID: Root=1-6805799c-4bec636328585df40b877bfc;3677aa35-6711-486a-9495-e78e9ac04c9b)\n","\n","Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json (Request ID: Root=1-680579ba-4e564a203585d88b41ca48dd;67aff31f-9a2d-4428-88f3-b305ae9de9fe)\n","\n","Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json (Request ID: Root=1-680579d8-532f93d65bd4c19908c2ca4c;76c8bb56-1937-4b9a-92f3-08905d4f1945)\n","\n","Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Fine-tuning finished.\n","***** train metrics *****\n","  epoch                    =      175.0\n","  total_flos               = 21390874GF\n","  train_loss               =     0.0593\n","  train_runtime            = 0:03:28.11\n","  train_samples            =       2900\n","  train_samples_per_second =     13.454\n","  train_steps_per_second   =      0.841\n","Training metrics saved.\n"]}],"source":["print(\"Starting fine-tuning...\")\n","train_result = trainer.train()\n","print(\"Fine-tuning finished.\")\n","\n","# --- Log Training Metrics ---\n","metrics = train_result.metrics\n","metrics[\"train_samples\"] = len(train_dataset)\n","trainer.log_metrics(\"train\", metrics)\n","trainer.save_metrics(\"train\", metrics)\n","print(\"Training metrics saved.\")"]},{"cell_type":"markdown","metadata":{"id":"SEIbmbP6v9Jp"},"source":["## 7. Save Adapter Locally\n","\n","Save the trained QLoRA adapter weights to the specified output directory."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"txrrlg0Tv9Jq","executionInfo":{"status":"ok","timestamp":1745189571549,"user_tz":-120,"elapsed":291,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"d735bec5-56eb-4160-aac2-5f7e8e831d77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Saving LoRA adapter to ./mistral-qlora-adapter_run4...\n","Adapter saved locally to ./mistral-qlora-adapter_run4\n","Tokenizer saved locally to ./mistral-qlora-adapter_run4\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/peft/utils/other.py:588: UserWarning: Unable to fetch remote file due to the following error 401 Client Error: Unauthorized for url: https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3/resolve/main/config.json (Request ID: Root=1-68057ac3-2b47b5667021e737386a3598;44651b5e-66f5-4a66-bde4-373979b4f728)\n","\n","Invalid credentials in Authorization header - silently ignoring the lookup for the file config.json in mistralai/Mistral-7B-Instruct-v0.3.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in mistralai/Mistral-7B-Instruct-v0.3 - will assume that the vocabulary was not modified.\n","  warnings.warn(\n"]}],"source":["print(f\"Saving LoRA adapter to {output_dir}...\")\n","trainer.save_model(output_dir) # Saves the adapter config and weights\n","print(f\"Adapter saved locally to {output_dir}\")\n","\n","# Optional: Save the tokenizer as well (good practice)\n","tokenizer.save_pretrained(output_dir)\n","print(f\"Tokenizer saved locally to {output_dir}\")"]},{"cell_type":"markdown","metadata":{"id":"TGx327_Sv9Jq"},"source":["## 8. (Optional) Push Adapter to Hugging Face Hub\n","\n","Push the trained adapter and tokenizer to your Hugging Face Hub repository for easy sharing and loading later."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176,"referenced_widgets":["11ee92e16c0e4fd08ed36ce6f9f5a06f","a26ba46a4e514c7bad64646dcbf02738","7309e39c96e24212b48fc87cea21945c","3cce6d6e25604bd5ab8209df11dbc9a3","125c175ec0694391af3657abf21cddde","18096d8cec4b432abbd7d00ae907298b","15cd50fc396042538102539163b50941","d7aa9687bb0d4ada9de3ee9f553ccdd1","7e5a33e57eb840efbb776e2ebf81a487","8975ac885057421282134ccb3d7a765a","d98e33a8bc80422b9242da17e645be3f"]},"id":"jI7A2Zbnv9Jq","executionInfo":{"status":"ok","timestamp":1745188361888,"user_tz":-120,"elapsed":3628,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"0ea7fc6e-ff80-4882-d493-f6fb3cdf7330"},"outputs":[{"output_type":"stream","name":"stdout","text":["Pushing adapter and tokenizer to Hugging Face Hub repo: EricBlv/mistral-7b-instruct-youtube-qlora...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/utils/hub.py:834: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["adapter_model.safetensors:   0%|          | 0.00/13.7M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ee92e16c0e4fd08ed36ce6f9f5a06f"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["No files have been modified since last commit. Skipping to prevent empty commit.\n","WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"]},{"output_type":"stream","name":"stdout","text":["Successfully pushed to Hub.\n"]}],"source":["# Make sure hf_hub_repo_id is set correctly\n","push_to_hub = True # Set to False if you don't want to push\n","\n","if push_to_hub:\n","    print(f\"Pushing adapter and tokenizer to Hugging Face Hub repo: {hf_hub_repo_id}...\")\n","    try:\n","        # Push the adapter (trainer saves adapter to output_dir)\n","        trainer.model.push_to_hub(hf_hub_repo_id, use_auth_token=True)\n","\n","        # Push the tokenizer\n","        tokenizer.push_to_hub(hf_hub_repo_id, use_auth_token=True)\n","\n","        print(\"Successfully pushed to Hub.\")\n","    except Exception as e:\n","        print(f\"Error pushing to Hub: {e}\")\n","else:\n","    print(\"Skipping push to Hugging Face Hub.\")"]},{"cell_type":"markdown","metadata":{"id":"dOP7f9kAv9Jq"},"source":["## 9. (Optional) Evaluation\n","\n","Perform evaluation on the held-out test set (if created) to calculate Perplexity and ROUGE scores."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"id":"NvPF7pltv9Jq","executionInfo":{"status":"ok","timestamp":1745189578783,"user_tz":-120,"elapsed":818,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"62ae252c-eae5-41ce-cff3-d2a799169da7"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Starting evaluation on the test set...\n","Running trainer.evaluate() for perplexity...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1/1 : < :]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Evaluation Loss (on test set): 3.7035\n","Perplexity (on test set): 40.5905\n","***** eval metrics *****\n","  epoch                   =      175.0\n","  eval_loss               =     3.7035\n","  eval_runtime            = 0:00:00.25\n","  eval_samples_per_second =      3.933\n","  eval_steps_per_second   =      3.933\n","Evaluation metrics saved.\n","\n","Attempting to calculate ROUGE score...\n","Loading ROUGE scorer using evaluate.load('rouge')...\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:evaluate.loading:Using the latest cached version of the module from /root/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--rouge/b01e0accf3bd6dd24839b769a5fda24e14995071570870922c71970b3a6ed886 (last modified on Sun Apr 20 22:27:59 2025) since it couldn't be found locally at evaluate-metric--rouge, or remotely on the Hugging Face Hub.\n"]},{"output_type":"stream","name":"stdout","text":["ROUGE scorer loaded successfully.\n","Skipping ROUGE score calculation because scorer failed to load.\n","\n","--- Training and Evaluation Complete ---\n","Adapter saved in: ./mistral-qlora-adapter_run4\n","Adapter pushed to: https://huggingface.co/EricBlv/mistral-7b-instruct-youtube-qlora\n"]}],"source":["import math\n","import torch # Ensure torch is imported if not already\n","import evaluate # Ensure evaluate is imported\n","\n","# Check if eval_dataset was loaded successfully earlier\n","if 'eval_dataset' in locals() and eval_dataset: # More robust check\n","    print(\"\\nStarting evaluation on the test set...\")\n","\n","    # --- Perplexity ---\n","    # The trainer.evaluate() function uses the eval_dataset passed during init\n","    try:\n","        # Ensure metrics dictionary exists from training results\n","        if 'metrics' not in locals():\n","             metrics = {} # Initialize if running eval separately\n","\n","        print(\"Running trainer.evaluate() for perplexity...\")\n","        eval_metrics = trainer.evaluate() # This runs evaluation on eval_dataset\n","        perplexity = math.exp(eval_metrics[\"eval_loss\"])\n","        print(f\"Evaluation Loss (on test set): {eval_metrics['eval_loss']:.4f}\")\n","        print(f\"Perplexity (on test set): {perplexity:.4f}\")\n","        # Save eval metrics\n","        metrics[\"eval_perplexity\"] = perplexity\n","        # Ensure trainer object exists before logging/saving\n","        if 'trainer' in locals():\n","            trainer.log_metrics(\"eval\", eval_metrics)\n","            trainer.save_metrics(\"eval\", eval_metrics)\n","            print(\"Evaluation metrics saved.\")\n","        else:\n","            print(\"Warning: Trainer object not found, skipping metric logging/saving.\")\n","    except Exception as e:\n","        print(f\"Could not calculate perplexity during evaluation: {e}\")\n","\n","    # --- ROUGE Score (More involved for generative tasks) ---\n","    # Requires generating predictions and comparing them to references.\n","\n","    # --- ROUGE Score ---\n","    print(\"\\nAttempting to calculate ROUGE score...\")\n","    rouge_scorer = None\n","    try:\n","        print(\"Loading ROUGE scorer using evaluate.load('rouge')...\")\n","        rouge_scorer = evaluate.load('rouge')\n","        print(\"ROUGE scorer loaded successfully.\")\n","    except Exception as e:\n","        # Print the full exception clearly\n","        print(\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n","        print(f\"!!! FAILED TO LOAD ROUGE SCORER !!!\")\n","        print(f\"!!! Error Type: {type(e).__name__}\")\n","        print(f\"!!! Error Details: {e}\")\n","        print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\\n\")\n","        print(\"Skipping ROUGE score calculation.\")\n","        # rouge_scorer remains None\n","        # rouge_scorer remains None\n","\n","    if rouge_scorer: # Proceed only if scorer loaded\n","        print(\"ROUGE scorer check passed.\") # Added print\n","        # Ensure the model and tokenizer are available\n","        if 'model' not in locals() or 'tokenizer' not in locals():\n","             print(\"Error: Model or tokenizer not found. Cannot generate predictions for ROUGE.\")\n","        else:\n","            print(\"Model and tokenizer found for ROUGE generation.\") # Added print\n","            # Ensure the model is in evaluation mode and on the correct device\n","            model.eval() # Ensure evaluation mode\n","            if torch.cuda.is_available():\n","                device = torch.device(\"cuda\")\n","            else:\n","                device = torch.device(\"cpu\")\n","            print(f\"Using device: {device} for ROUGE generation.\") # Added print\n","            # Ensure model is on device (trainer usually handles this, but safe check)\n","            try:\n","                model.to(device)\n","            except Exception as e:\n","                print(f\"Warning: Could not move model to device {device}: {e}\")\n","\n","\n","            all_preds = []\n","            all_labels = []\n","\n","            # Prepare inputs and get references (outputs) from the eval_dataset (test.jsonl)\n","            eval_batch_size = 4 # Adjust based on GPU memory\n","            print(f\"Generating predictions for {len(eval_dataset)} test samples...\")\n","            for i in range(0, len(eval_dataset), eval_batch_size):\n","                print(f\"\\nProcessing batch starting at index {i}...\") # Added print for loop entry\n","                # Get batch indices\n","                indices = range(i, min(i + eval_batch_size, len(eval_dataset)))\n","                # Select dictionary items for the batch using indices\n","                # This ensures we get dictionaries even if direct iteration yields strings\n","                try:\n","                    batch_samples_dicts = [eval_dataset[idx] for idx in indices]\n","                    print(f\"  Successfully accessed batch samples for indices {indices}.\") # Added print\n","                except Exception as e:\n","                     print(f\"  Error accessing eval_dataset batch at index {i}: {e}. Skipping batch.\")\n","                     continue # Skip this batch if dataset access fails\n","\n","                # Extract the prompt part (instruction + input) for generation\n","                prompts = []\n","                labels = []\n","                try:\n","                    for sample_idx, sample in enumerate(batch_samples_dicts): # Iterate over the list of dictionaries\n","                        instruction = sample['instruction'] # Should work now\n","                        context = sample['input']\n","                        response = sample['output'] # This is the reference\n","                        prompt_text = f\"<s>[INST] {instruction}\\n---\\n{context}\\n--- [/INST]\" # Match training format\n","                        prompts.append(prompt_text)\n","                        labels.append(response) # The reference LLM-generated answer\n","                    print(f\"  Successfully processed prompts and labels for batch.\") # Added print\n","                except KeyError as e:\n","                     print(f\"  Error: Missing key {e} in eval_dataset sample: {sample}. Skipping batch.\")\n","                     continue # Skip batch if data format is wrong\n","                except Exception as e:\n","                     print(f\"  Unexpected error processing batch sample {sample}: {e}. Skipping batch.\")\n","                     continue\n","\n","                # Tokenize prompts\n","                try:\n","                    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(device)\n","                    print(f\"  Successfully tokenized prompts for batch.\") # Added print\n","                except Exception as e:\n","                    print(f\"  Error tokenizing prompts for batch {i}: {e}. Skipping batch.\")\n","                    continue\n","\n","                # Generate predictions\n","                # Use the final model (potentially PEFT model)\n","                current_model_for_generation = getattr(trainer, 'model', model) # Use trainer's model if available\n","                try:\n","                    print(f\"  Generating predictions for batch...\") # Added print\n","                    with torch.no_grad():\n","                         # Adjust generation parameters as needed\n","                        outputs = current_model_for_generation.generate(\n","                            **inputs,\n","                            max_new_tokens=max_seq_length, # Allow generating up to max length\n","                            eos_token_id=tokenizer.eos_token_id,\n","                            pad_token_id=tokenizer.pad_token_id, # Explicitly set pad_token_id\n","                            do_sample=False, # Use greedy decoding for reproducible evaluation\n","                            num_beams=1 # for greedy\n","                        )\n","                    print(f\"  Successfully generated predictions for batch.\") # Added print\n","                except Exception as e:\n","                     print(f\"  Error during model.generate for batch {i}: {e}. Skipping batch.\")\n","                     continue\n","\n","                # Decode generated sequences\n","                # Important: Decode *only the generated part*, not the prompt\n","                preds_decoded = []\n","                try:\n","                    print(f\"  Decoding predictions for batch...\") # Added print\n","                    for idx, output_tokens in enumerate(outputs):\n","                        # Find the end of the prompt tokens in the output\n","                        input_token_len = len(inputs[\"input_ids\"][idx])\n","                        # Handle potential padding tokens in input_ids if tokenizer pads left\n","                        if tokenizer.padding_side == 'left':\n","                           # Ensure attention mask sum is calculated correctly\n","                           mask_sum = inputs[\"attention_mask\"][idx].sum().item()\n","                           input_token_len = int(mask_sum) # Cast to int just in case\n","\n","                        generated_tokens = output_tokens[input_token_len:]\n","                        pred = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n","                        preds_decoded.append(pred.strip())\n","                    print(f\"  Successfully decoded predictions for batch.\") # Added print\n","                except Exception as e:\n","                    print(f\"  Error decoding predictions for batch {i}: {e}. Skipping batch.\")\n","                    continue # Skip batch if decoding fails\n","\n","                all_preds.extend(preds_decoded)\n","                all_labels.extend(labels)\n","\n","                # Removed progress print from here to avoid clutter, main check is loop entry\n","\n","            # Compute ROUGE after processing all batches\n","            print(\"\\nFinished processing all batches for ROUGE.\") # Added print\n","            if len(all_preds) == len(all_labels) and len(all_preds) > 0: # Ensure we have pairs to compare\n","                try:\n","                    print(f\"Computing ROUGE score for {len(all_preds)} generated predictions...\")\n","                    rouge_results = rouge_scorer.compute(\n","                        predictions=all_preds,\n","                        references=all_labels\n","                    )\n","                    print(\"\\nROUGE Scores (on test set):\")\n","                    print(rouge_results)\n","\n","                    # Add ROUGE-L to metrics dictionary\n","                    if 'rougeL' in rouge_results:\n","                        metrics[\"eval_rougeL\"] = rouge_results['rougeL']\n","                        # Log and save updated metrics if trainer exists\n","                        if 'trainer' in locals():\n","                            trainer.log_metrics(\"eval\", {\"rougeL\": rouge_results['rougeL']})\n","                            trainer.save_metrics(\"eval\", metrics) # Save combined eval metrics\n","                            print(\"Evaluation metrics including ROUGE saved.\")\n","                        else:\n","                             print(\"Warning: Trainer object not found, skipping ROUGE metric logging/saving.\")\n","\n","                except Exception as e:\n","                    print(f\"Could not compute or save ROUGE scores: {e}\")\n","                    # Print examples even if scoring fails\n","                    print(\"\\nExample Prediction:\", all_preds[0] if all_preds else \"N/A\")\n","                    print(\"Example Reference:\", all_labels[0] if all_labels else \"N/A\")\n","            else:\n","                 print(\"\\nWarning: No valid prediction/label pairs generated. Cannot compute ROUGE score.\")\n","                 if len(all_preds) != len(all_labels):\n","                      print(f\"Mismatch in length: Predictions={len(all_preds)}, Labels={len(all_labels)}\")\n","\n","    else: # rouge_scorer is None\n","         print(\"Skipping ROUGE score calculation because scorer failed to load.\")\n","\n","\n","else:\n","    print(\"\\nNo evaluation dataset loaded ('eval_dataset' not found or is None). Skipping evaluation step.\")\n","\n","print(\"\\n--- Training and Evaluation Complete ---\")\n","\n","# Final check for output directory and optional Hub push info\n","if 'output_dir' in locals():\n","    print(f\"Adapter saved in: {output_dir}\")\n","if 'push_to_hub' in locals() and push_to_hub and 'hf_hub_repo_id' in locals() and hf_hub_repo_id:\n","    print(f\"Adapter pushed to: https://huggingface.co/{hf_hub_repo_id}\")\n","\n","# Clean up memory (important in Colab)\n","# Consider uncommenting these if you face memory issues later\n","# print(\"\\nAttempting memory cleanup...\")\n","# try:\n","#     del model\n","#     del trainer\n","#     import gc\n","#     gc.collect()\n","#     torch.cuda.empty_cache()\n","#     print(\"Memory cleanup attempted.\")\n","# except NameError:\n","#     print(\"Model or trainer not defined, skipping specific cleanup.\")\n","# except Exception as e:\n","#      print(f\"Error during memory cleanup: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"cFFqfchSv9Jq"},"source":["## 10. Download Adapter\n","\n","If you want to download the adapter directly from Colab, you can zip the output directory."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oV5jIrP6v9Jq","executionInfo":{"status":"ok","timestamp":1745188512883,"user_tz":-120,"elapsed":28474,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"03577a04-4e77-428c-c914-5c501d28001f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Zipping adapter directory: ./mistral-qlora-adapter_run3 -> mistral-qlora-adapter_run3.zip\n","Adapter zipped to mistral-qlora-adapter_run3.zip\n"]}],"source":["import shutil\n","\n","adapter_zip_name = f\"{os.path.basename(output_dir)}\"\n","# Check if the directory exists before zipping\n","if os.path.isdir(output_dir):\n","    print(f\"Zipping adapter directory: {output_dir} -> {adapter_zip_name}.zip\")\n","    shutil.make_archive(adapter_zip_name, 'zip', output_dir)\n","    print(f\"Adapter zipped to {adapter_zip_name}.zip\")\n","    # You can now download this zip file from the Colab file browser\n","else:\n","    print(f\"Output directory {output_dir} not found. Cannot create zip file.\")"]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ibf2k1aAbH2o","executionInfo":{"status":"ok","timestamp":1745189897587,"user_tz":-120,"elapsed":105,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"eb8647e7-5401-4bf3-b76e-748641949398"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mbitsandbytes\u001b[0m/                   \u001b[01;34mmistral-qlora-adapter_run4\u001b[0m/\n","\u001b[01;34mmistral-qlora-adapter\u001b[0m/          mistral_qlora_youtube.ipynb\n","\u001b[01;34mmistral-qlora-adapter_run2\u001b[0m/     test.jsonl\n","\u001b[01;34mmistral-qlora-adapter_run3\u001b[0m/     train.jsonl\n","mistral-qlora-adapter_run3.zip\n"]}]},{"cell_type":"code","source":["cd mistral-qlora-adapter_run3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"62cOv_FLbxtw","executionInfo":{"status":"ok","timestamp":1745190075865,"user_tz":-120,"elapsed":46,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"663ceaa6-8a1a-4e30-fe9f-342ff8512836"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/mistral_finetuning/mistral-qlora-adapter_run3\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VZglGD2Sb0cW","executionInfo":{"status":"ok","timestamp":1745190080191,"user_tz":-120,"elapsed":106,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"5bc66a77-269e-4a4c-badd-04edbdf62887"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["adapter_config.json        \u001b[0m\u001b[01;34mcheckpoint-25\u001b[0m/           tokenizer_config.json\n","adapter_model.safetensors  \u001b[01;34mcheckpoint-50\u001b[0m/           tokenizer.json\n","all_results.json           \u001b[01;34mcheckpoint-75\u001b[0m/           tokenizer.model\n","\u001b[01;34mcheckpoint-100\u001b[0m/            eval_results.json        training_args.bin\n","\u001b[01;34mcheckpoint-125\u001b[0m/            README.md                train_results.json\n","\u001b[01;34mcheckpoint-150\u001b[0m/            special_tokens_map.json\n"]}]},{"cell_type":"code","source":["cd .."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BqQ0x4okcF2u","executionInfo":{"status":"ok","timestamp":1745190152390,"user_tz":-120,"elapsed":14,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"b80003fa-0af2-4159-dfea-f1266488d51d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/mistral_finetuning\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfEBGutQg9IY","executionInfo":{"status":"ok","timestamp":1745192266729,"user_tz":-120,"elapsed":107,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"c1bea3d8-bb73-4120-a19e-e3ac5d8f4df4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mbitsandbytes\u001b[0m/                   \u001b[01;34mmistral-qlora-adapter_run4\u001b[0m/\n","\u001b[01;34mmistral-qlora-adapter\u001b[0m/          mistral_qlora_youtube.ipynb\n","\u001b[01;34mmistral-qlora-adapter_run2\u001b[0m/     test.jsonl\n","\u001b[01;34mmistral-qlora-adapter_run3\u001b[0m/     train.jsonl\n","mistral-qlora-adapter_run3.zip\n"]}]},{"cell_type":"code","source":["# merge_adapter.py\n","import torch\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from peft import PeftModel\n","import os\n","import shutil\n","\n","# --- Configuration ---\n","base_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n","# *** CONFIRM this is the adapter path from your BEST run ***\n","# Use the absolute path as determined before\n","adapter_path = \"/content/drive/MyDrive/mistral_finetuning/mistral-qlora-adapter_run3\"\n","# *** This directory will be created in Colab ***\n","merged_model_path = \"./merged_mistral_adapter\"\n","\n","# --- Ensure paths exist ---\n","if not os.path.isdir(adapter_path):\n","    print(f\"Error: Adapter path not found: {adapter_path}\")\n","    print(\"Please ensure the adapter files are in the correct directory.\")\n","    exit(1)\n","\n","print(f\"Loading CLEAN base model for merging: {base_model_name}\")\n","# Load base model WITHOUT quantization for merging\n","# device_map='auto' should still work on Colab GPU\n","base_model = AutoModelForCausalLM.from_pretrained(\n","    base_model_name,\n","    torch_dtype=torch.bfloat16, # Use bf16 for efficiency on Colab GPU\n","    # quantization_config=None, # Ensure no quantization config is passed\n","    device_map=\"auto\",\n","    trust_remote_code=True,\n",")\n","print(\"Clean base model loaded.\")\n","\n","print(f\"Loading adapter: {adapter_path}\")\n","# Load the LoRA adapter onto the CLEAN base model\n","model = PeftModel.from_pretrained(base_model, adapter_path)\n","print(\"Adapter loaded onto clean base model.\")\n","\n","print(\"Merging adapter...\")\n","# Merge the adapter weights into the base model\n","model = model.merge_and_unload()\n","print(\"Merge complete.\")\n","\n","print(f\"Saving merged model to: {merged_model_path}\")\n","# Ensure target directory exists\n","os.makedirs(merged_model_path, exist_ok=True)\n","\n","# Save the merged model (should work now without meta tensors)\n","try:\n","    model.save_pretrained(merged_model_path)\n","except NotImplementedError as e:\n","     print(f\"ERROR during save_pretrained: {e}\")\n","     print(\"This might indicate the merge didn't fully resolve meta tensors.\")\n","     print(\"Consider saving state_dict manually or further debugging.\")\n","     exit(1)\n","except Exception as e:\n","     print(f\"An unexpected error occurred during save_pretrained: {e}\")\n","     exit(1)\n","\n","\n","print(\"Loading tokenizer...\")\n","# Load and save the tokenizer associated with the base model\n","tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n","tokenizer.save_pretrained(merged_model_path)\n","\n","print(\"Merged model and tokenizer saved successfully.\")\n","\n","# Optional: Clean up memory if needed in Colab\n","import gc\n","del model\n","del base_model\n","gc.collect()\n","if torch.cuda.is_available(): torch.cuda.empty_cache()\n","print(\"Memory cleanup attempted.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":444,"referenced_widgets":["16ab91dba5e840dabd04829bc72b6f12","874af6f47525400293d3bf823ac3c3c7","f10d53f41c5143dcb35fcebc91b459c7","5eb1b87b290e4c96b69368862b3c34e5","c0d4c959131c4394b0e42035ce5a371a","d53625e390c14c16a0c3891b293532a8","c7897643241a4e3192581825ff069058","527b1c0a48d145a69ebc17cb8abf99fb","8d081fa5404a45c9b759a4a820dcf94d","36eb6d87ff5e4feb815fc4e1bbcc9cd3","18cd0469e24645c7b56864f1f70269b7"]},"id":"zV1eaGyCg2_X","executionInfo":{"status":"ok","timestamp":1745192452763,"user_tz":-120,"elapsed":103791,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"557a9899-77f8-4ddd-f5cf-1fcd2bcaabe0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading CLEAN base model for merging: mistralai/Mistral-7B-Instruct-v0.3\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ab91dba5e840dabd04829bc72b6f12"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Clean base model loaded.\n","Loading adapter: /content/drive/MyDrive/mistral_finetuning/mistral-qlora-adapter_run3\n","Adapter loaded onto clean base model.\n","Merging adapter...\n","Merge complete.\n","Saving merged model to: ./merged_mistral_adapter\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"]},{"output_type":"stream","name":"stdout","text":["Loading tokenizer...\n","Merged model and tokenizer saved successfully.\n","Memory cleanup attempted.\n"]}]},{"cell_type":"code","source":["    !ls -lh ./merged_mistral_adapter/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AU-6rBO7amB-","executionInfo":{"status":"ok","timestamp":1745192461412,"user_tz":-120,"elapsed":1316,"user":{"displayName":"Eric","userId":"00528069777177458091"}},"outputId":"0da91b1a-4a76-4244-edac-1d60e531d70a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["total 14G\n","-rw------- 1 root root  653 Apr 20 23:39 config.json\n","-rw------- 1 root root  111 Apr 20 23:39 generation_config.json\n","-rw------- 1 root root 4.7G Apr 20 23:39 model-00001-of-00003.safetensors\n","-rw------- 1 root root 4.7G Apr 20 23:39 model-00002-of-00003.safetensors\n","-rw------- 1 root root 4.3G Apr 20 23:40 model-00003-of-00003.safetensors\n","-rw------- 1 root root  24K Apr 20 23:40 model.safetensors.index.json\n","-rw------- 1 root root  414 Apr 20 23:40 special_tokens_map.json\n","-rw------- 1 root root 138K Apr 20 23:40 tokenizer_config.json\n","-rw------- 1 root root 1.9M Apr 20 23:40 tokenizer.json\n","-rw------- 1 root root 574K Apr 20 23:40 tokenizer.model\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qthMBnfZjy68"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.16"},"colab":{"provenance":[],"gpuType":"A100"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f0eab43e78bc4e1bb11c132ed394ea4a":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":[],"layout":"IPY_MODEL_1ec8b066412048a0ad1173da5b02c954"}},"6c76c1cbe08e4eaa928b09d39f3c02fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a240cb7b880d484792c7940717392856","placeholder":"​","style":"IPY_MODEL_f14f3e7e81194f708ee1251351cd7111","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"9ce120b5149a424eafe76ac9c36782ba":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_8104f2a1b1d44ff39f312a40636b6053","placeholder":"​","style":"IPY_MODEL_554ed093c5bf4d79971c72066da71acf","value":""}},"28e73934ba0b44aeaf3c38e56e67a224":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_fc8eec59c13449268a528a60c3453ccd","style":"IPY_MODEL_1a250eb351a44aca9003b46dbdf98267","value":true}},"057c4a9963c44da79289fcba1c903a76":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_07f640a537124fb1bfd1318a656dd9b8","style":"IPY_MODEL_f70c2b87e6d14b40932996b7a02e40af","tooltip":""}},"7bf781c2027f463189959077a3f9ef0c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9bcecae5240d452dbe2e298e8517fe0c","placeholder":"​","style":"IPY_MODEL_9d4c50b6589b46e684a819c2ee0bdfef","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"1ec8b066412048a0ad1173da5b02c954":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"a240cb7b880d484792c7940717392856":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f14f3e7e81194f708ee1251351cd7111":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8104f2a1b1d44ff39f312a40636b6053":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"554ed093c5bf4d79971c72066da71acf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc8eec59c13449268a528a60c3453ccd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a250eb351a44aca9003b46dbdf98267":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07f640a537124fb1bfd1318a656dd9b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f70c2b87e6d14b40932996b7a02e40af":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"9bcecae5240d452dbe2e298e8517fe0c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d4c50b6589b46e684a819c2ee0bdfef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc32f186451f4ecfbdd8e0460786144d":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d705d1895ea14e849eff042d9ac7fa3b","placeholder":"​","style":"IPY_MODEL_1a98b211695947b2832bd7abfb5d397c","value":"Connecting..."}},"d705d1895ea14e849eff042d9ac7fa3b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a98b211695947b2832bd7abfb5d397c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96b1bb79d4cf4f91b4edab9ccfbb976b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e3fe2a8a5c35422196893ef8521daef2","IPY_MODEL_a98a019d2d1d46ef80e590a69a903d94","IPY_MODEL_48cbaf251f924c99a22321ae143adce0"],"layout":"IPY_MODEL_2e81d12660a74bf1b7174e277b4c0b1b"}},"e3fe2a8a5c35422196893ef8521daef2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_258fb06e9dc34b0eb47aed1b783d2861","placeholder":"​","style":"IPY_MODEL_9ec020310c3d4f5ba05f453d13b2ce86","value":"Loading checkpoint shards: 100%"}},"a98a019d2d1d46ef80e590a69a903d94":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9152e89d5eee4c16b39327d584305679","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_810f48a49f384d968219efa7ba5d80b3","value":3}},"48cbaf251f924c99a22321ae143adce0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_057e9950cd1946909832069f5691dc60","placeholder":"​","style":"IPY_MODEL_257575efec914a469283c0658e4ae333","value":" 3/3 [00:08&lt;00:00,  2.94s/it]"}},"2e81d12660a74bf1b7174e277b4c0b1b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"258fb06e9dc34b0eb47aed1b783d2861":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ec020310c3d4f5ba05f453d13b2ce86":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9152e89d5eee4c16b39327d584305679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"810f48a49f384d968219efa7ba5d80b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"057e9950cd1946909832069f5691dc60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"257575efec914a469283c0658e4ae333":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11ee92e16c0e4fd08ed36ce6f9f5a06f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a26ba46a4e514c7bad64646dcbf02738","IPY_MODEL_7309e39c96e24212b48fc87cea21945c","IPY_MODEL_3cce6d6e25604bd5ab8209df11dbc9a3"],"layout":"IPY_MODEL_125c175ec0694391af3657abf21cddde"}},"a26ba46a4e514c7bad64646dcbf02738":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18096d8cec4b432abbd7d00ae907298b","placeholder":"​","style":"IPY_MODEL_15cd50fc396042538102539163b50941","value":"adapter_model.safetensors: 100%"}},"7309e39c96e24212b48fc87cea21945c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d7aa9687bb0d4ada9de3ee9f553ccdd1","max":13665592,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e5a33e57eb840efbb776e2ebf81a487","value":13665592}},"3cce6d6e25604bd5ab8209df11dbc9a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8975ac885057421282134ccb3d7a765a","placeholder":"​","style":"IPY_MODEL_d98e33a8bc80422b9242da17e645be3f","value":" 13.7M/13.7M [00:01&lt;00:00, 42.9MB/s]"}},"125c175ec0694391af3657abf21cddde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18096d8cec4b432abbd7d00ae907298b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15cd50fc396042538102539163b50941":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d7aa9687bb0d4ada9de3ee9f553ccdd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e5a33e57eb840efbb776e2ebf81a487":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8975ac885057421282134ccb3d7a765a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d98e33a8bc80422b9242da17e645be3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16ab91dba5e840dabd04829bc72b6f12":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_874af6f47525400293d3bf823ac3c3c7","IPY_MODEL_f10d53f41c5143dcb35fcebc91b459c7","IPY_MODEL_5eb1b87b290e4c96b69368862b3c34e5"],"layout":"IPY_MODEL_c0d4c959131c4394b0e42035ce5a371a"}},"874af6f47525400293d3bf823ac3c3c7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d53625e390c14c16a0c3891b293532a8","placeholder":"​","style":"IPY_MODEL_c7897643241a4e3192581825ff069058","value":"Loading checkpoint shards: 100%"}},"f10d53f41c5143dcb35fcebc91b459c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_527b1c0a48d145a69ebc17cb8abf99fb","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d081fa5404a45c9b759a4a820dcf94d","value":3}},"5eb1b87b290e4c96b69368862b3c34e5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_36eb6d87ff5e4feb815fc4e1bbcc9cd3","placeholder":"​","style":"IPY_MODEL_18cd0469e24645c7b56864f1f70269b7","value":" 3/3 [00:05&lt;00:00,  1.69s/it]"}},"c0d4c959131c4394b0e42035ce5a371a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d53625e390c14c16a0c3891b293532a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7897643241a4e3192581825ff069058":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"527b1c0a48d145a69ebc17cb8abf99fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d081fa5404a45c9b759a4a820dcf94d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"36eb6d87ff5e4feb815fc4e1bbcc9cd3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18cd0469e24645c7b56864f1f70269b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}