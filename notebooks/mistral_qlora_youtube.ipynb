{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Mistral-7B-Instruct with QLoRA on YouTube Transcripts\n",
    "\n",
    "This notebook performs QLoRA fine-tuning on `mistralai/Mistral-7B-Instruct-v0.3` using a dataset derived from YouTube video transcripts.\n",
    "\n",
    "**Steps:**\n",
    "1. Installs necessary libraries.\n",
    "2. Sets up Hugging Face Hub authentication.\n",
    "3. Loads and prepares the dataset (`train.jsonl`).\n",
    "4. Configures the QLoRA parameters and loads the base model in 4-bit.\n",
    "5. Sets up the `SFTTrainer` from the TRL library.\n",
    "6. Runs the fine-tuning process.\n",
    "7. Saves the trained LoRA adapter locally.\n",
    "8. (Optional) Pushes the adapter to the Hugging Face Hub.\n",
    "9. (Optional) Performs basic evaluation (Perplexity, ROUGE-L)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installs\n",
    "\n",
    "Install the required libraries. `bitsandbytes` requires a specific version compatible with Colab's GPU environment (usually T4 or A100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets accelerate peft trl bitsandbytes sentencepiece py7zr torch ninja huggingface_hub evaluate rouge_score pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hugging Face Hub Authentication\n",
    "\n",
    "Log in to Hugging Face Hub to save the adapter and potentially download gated models. You'll need a User Access Token with `write` permissions.\n",
    "\n",
    "Get your token here: https://huggingface.co/settings/tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3374848e9824469c82be2f0e994222ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login, notebook_login\n",
    "# Use notebook_login() for interactive login in Colab/Jupyter\n",
    "# or login(\"YOUR_HF_TOKEN\") if running in a script\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Prepare Dataset\n",
    "\n",
    "Upload your `train.jsonl` file (generated by `data_gen.py`) to your Colab session. You can do this using the file browser on the left panel.\n",
    "\n",
    "Alternatively, if you've pushed it to a Hugging Face dataset repository, you can load it directly from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset, DatasetDict \u001b[38;5;66;03m# Import DatasetDict\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m      6\u001b[0m     AutoTokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     logging,\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset, DatasetDict # Import DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import evaluate # For ROUGE score\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "# Model and Tokenizer\n",
    "base_model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Dataset paths (ensure these files are uploaded to Colab)\n",
    "train_dataset_path = \"train.jsonl\"\n",
    "test_dataset_path = \"test.jsonl\" # Path to the test split\n",
    "\n",
    "# Option 2: Load from Hugging Face Hub (replace with your repo ID if you pushed the dataset)\n",
    "# dataset_hub_id = \"your_username/your_dataset_repo_name\"\n",
    "# dataset_files = {\"train\": \"train.jsonl\", \"test\": \"test.jsonl\"}\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", # Recommended\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Use bfloat16 for faster training\n",
    "    bnb_4bit_use_double_quant=True, # Recommended\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,                 # LoRA attention dimension (rank)\n",
    "    lora_alpha=16,       # Alpha parameter for scaling\n",
    "    lora_dropout=0.05,   # Dropout probability for LoRA layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[ # Find target modules using script below or common sense\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        # \"gate_proj\", # Optional\n",
    "        # \"up_proj\",   # Optional\n",
    "        # \"down_proj\", # Optional\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "output_dir = \"./mistral-qlora-adapter\" # Local directory to save adapter\n",
    "per_device_train_batch_size = 2\n",
    "gradient_accumulation_steps = 8\n",
    "# num_train_epochs = 1.0 # Can use epochs or max_steps\n",
    "max_steps = 300 # Adjust based on dataset size and desired training time (~200-400 recommended)\n",
    "learning_rate = 2e-4\n",
    "optim = \"paged_adamw_32bit\" # Recommended optimizer for QLoRA\n",
    "logging_steps = 25\n",
    "save_steps = 50 # Save checkpoints periodically\n",
    "max_grad_norm = 0.3\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"constant\" # Or \"cosine\", \"linear\"\n",
    "evaluation_strategy = \"steps\" # Evaluate during training using the test set\n",
    "eval_steps = 50             # Evaluate every N steps\n",
    "# report_to=\"tensorboard\" # Or wandb\n",
    "\n",
    "# SFT Trainer specific\n",
    "max_seq_length = MAX_CHUNK_TOKENS = 512 # Defined in data_gen.py, ensure consistency\n",
    "packing = False # Set to True if you want to pack sequences, requires more memory\n",
    "\n",
    "# Hugging Face Hub repo ID (optional)\n",
    "hf_hub_repo_id = \"your_username/mistral-7b-instruct-youtube-qlora\" # CHANGE THIS to your HF username/repo name\n",
    "\n",
    "# --- Load Dataset ---\n",
    "train_dataset = None\n",
    "eval_dataset = None\n",
    "\n",
    "try:\n",
    "    # Check if local files exist\n",
    "    if os.path.exists(train_dataset_path) and os.path.exists(test_dataset_path):\n",
    "        print(f\"Loading dataset from local files: {train_dataset_path}, {test_dataset_path}\")\n",
    "        # Load both files into a DatasetDict\n",
    "        dataset = load_dataset('json', data_files={'train': train_dataset_path, 'test': test_dataset_path})\n",
    "        train_dataset = dataset['train']\n",
    "        eval_dataset = dataset['test'] # Use the 'test' split for evaluation\n",
    "        print(f\"Datasets loaded: Train size={len(train_dataset)}, Eval size={len(eval_dataset)}\")\n",
    "    # elif dataset_hub_id: # Option to load from Hub\n",
    "    #     print(f\"Local files not found. Attempting to load from Hub: {dataset_hub_id}\")\n",
    "    #     dataset = load_dataset(dataset_hub_id, data_files=dataset_files)\n",
    "    #     train_dataset = dataset['train']\n",
    "    #     eval_dataset = dataset['test']\n",
    "    #     print(f\"Datasets loaded from Hub: Train size={len(train_dataset)}, Eval size={len(eval_dataset)}\")\n",
    "    else:\n",
    "        missing_files = []\n",
    "        if not os.path.exists(train_dataset_path): missing_files.append(train_dataset_path)\n",
    "        if not os.path.exists(test_dataset_path): missing_files.append(test_dataset_path)\n",
    "        raise FileNotFoundError(f\"Dataset file(s) not found. Please upload: {', '.join(missing_files)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    # Stop execution if datasets aren't loaded\n",
    "    # exit()\n",
    "\n",
    "# Ensure evaluation strategy is set correctly if eval_dataset exists\n",
    "if eval_dataset is None:\n",
    "    evaluation_strategy = \"no\"\n",
    "    eval_steps = None\n",
    "    print(\"Warning: No evaluation dataset loaded. Disabling evaluation during training.\")\n",
    "else:\n",
    "    # Keep evaluation_strategy and eval_steps as defined earlier\n",
    "    print(\"Evaluation dataset loaded. Evaluation during training is enabled.\")\n",
    "\n",
    "\n",
    "# --- Format dataset for SFTTrainer ---\n",
    "# Mistral Instruct format:\n",
    "# <s>[INST] Instruction [/INST] Answer </s>\n",
    "# We need a function that takes a sample and returns a formatted string.\n",
    "\n",
    "def format_instruction(sample):\n",
    "    # Uses the 'instruction', 'input', and 'output' fields from train.jsonl/test.jsonl\n",
    "    # 'input' contains the original transcript chunk\n",
    "    # 'output' contains the LLM-generated answer\n",
    "    instruction = sample['instruction']\n",
    "    context = sample['input'] # The transcript chunk\n",
    "    response = sample['output'] # The LLM-generated answer\n",
    "\n",
    "    # Combine instruction and context for the prompt\n",
    "    prompt = f\"{instruction}\\n---\\n{context}\\n---\" # Separators help delineate\n",
    "\n",
    "    # Format according to Mistral Instruct template\n",
    "    return f\"<s>[INST] {prompt} [/INST] {response} </s>\"\n",
    "\n",
    "print(\"Dataset formatting function defined.\")\n",
    "# Example of formatted text:\n",
    "if train_dataset and len(train_dataset) > 0:\n",
    "    print(\"\\nExample formatted training sample:\")\n",
    "    print(format_instruction(train_dataset[0]))\n",
    "else:\n",
    "    print(\"Train dataset is empty or not loaded, cannot show example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Model and Tokenizer with QLoRA Config\n",
    "\n",
    "Load the base model (`Mistral-7B-Instruct-v0.3`) with 4-bit quantization using the `BitsAndBytesConfig`. We also load the corresponding tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Set pad token\n",
    "tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n",
    "print(\"Tokenizer loaded.\")\n",
    "\n",
    "# Load Model with QLoRA config\n",
    "print(f\"Loading base model: {base_model_name} with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\", # Automatically map layers to GPU\n",
    "    trust_remote_code=True, # Necessary for some models\n",
    "    # torch_dtype=torch.bfloat16, # dtype is set in bnb_config\n",
    ")\n",
    "print(\"Base model loaded.\")\n",
    "\n",
    "# --- Sanity Check: Find LoRA Target Modules ---\n",
    "# Uncomment the following lines to see all linear layer names\n",
    "# This helps verify the `target_modules` in LoraConfig\n",
    "# print(\"\\nModel Architecture:\")\n",
    "# print(model)\n",
    "# print(\"\\nFinding potential LoRA target modules (Linear layers):\")\n",
    "# linear_layers = set()\n",
    "# for name, module in model.named_modules():\n",
    "#     if isinstance(module, torch.nn.Linear):\n",
    "#          #Focus on layers typically targeted by LoRA in transformers\n",
    "#          if any(layer_name in name for layer_name in ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']):\n",
    "#              # Get the last part of the name (e.g., 'q_proj')\n",
    "#              layer_name = name.split('.')[-1]\n",
    "#              linear_layers.add(layer_name)\n",
    "# print(f\"Found linear layer names: {linear_layers}\")\n",
    "# print(f\"Using target modules: {peft_config.target_modules}\")\n",
    "# print(\"Ensure these match the typical layers for Mistral architecture.\")\n",
    "\n",
    "# --- Prepare model for k-bit training ---\n",
    "# Cast layer norms and head to fp32 for stability\n",
    "# model = prepare_model_for_kbit_training(model) # TRL's SFTTrainer handles this\n",
    "\n",
    "# --- Create PEFT Model ---\n",
    "# Note: SFTTrainer can also handle PEFT model creation if peft_config is passed\n",
    "# Creating it explicitly here for clarity\n",
    "# print(\"\\nApplying LoRA adapter to the base model...\")\n",
    "# model = get_peft_model(model, peft_config)\n",
    "# print(\"LoRA adapter applied.\")\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "# Configure cache usage (optional, but recommended)\n",
    "model.config.use_cache = False # Important for training stability with gradient checkpointing\n",
    "# model.config.pretraining_tp = 1 # If you face tensor parallelism issues\n",
    "print(\"Model prepared for training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Configure SFTTrainer\n",
    "\n",
    "We use the `SFTTrainer` from the TRL library, which simplifies the process of supervised fine-tuning for instruction-following tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_steps=save_steps,\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    # num_train_epochs=num_train_epochs,\n",
    "    max_steps=max_steps,\n",
    "    fp16=False, # Use bf16 if available (Ampere GPUs like A100)\n",
    "    bf16=True, # Set to True for Ampere GPUs, False for T4 (if bnb_compute_dtype is bfloat16)\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=True, # Speeds up training by grouping similar length sequences\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    # Evaluation settings (only if eval_dataset is provided)\n",
    "    evaluation_strategy=evaluation_strategy,\n",
    "    eval_steps=eval_steps,\n",
    "    # report_to=report_to,\n",
    "    # Pushing to Hub options\n",
    "    # push_to_hub=True, # Set to True to push model/adapter during training\n",
    "    # hub_model_id=hf_hub_repo_id, # Repository name on Hugging Face Hub\n",
    "    # hub_strategy=\"checkpoint\", # Push on every save\n",
    "    # hub_token=os.getenv(\"HF_TOKEN\") # Use token stored in environment or login()\n",
    ")\n",
    "\n",
    "print(\"Training Arguments configured.\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset, # Pass evaluation dataset here\n",
    "    peft_config=peft_config, # Pass PEFT config here\n",
    "    # dataset_text_field=\"text\", # Use if you pre-formatted into a 'text' column\n",
    "    formatting_func=format_instruction, # Pass the formatting function\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    packing=packing,\n",
    ")\n",
    "\n",
    "print(\"SFTTrainer initialized.\")\n",
    "# TRL automatically handles prepare_model_for_kbit_training when peft_config is passed\n",
    "# model.print_trainable_parameters()\n",
    "\n",
    "# Verify bf16 setting based on GPU availability\n",
    "if torch.cuda.is_bf16_supported():\n",
    "    print(\"\\nBF16 is supported. Training will use BF16.\")\n",
    "    if not training_arguments.bf16:\n",
    "      print(\"Warning: BF16 supported but not enabled in TrainingArguments. Enabling it.\")\n",
    "      training_arguments.bf16 = True\n",
    "      training_arguments.fp16 = False # Ensure fp16 is off if bf16 is on\n",
    "else:\n",
    "    print(\"\\nBF16 is NOT supported. Ensure compute_dtype in BitsAndBytesConfig is appropriate (e.g., float16) and bf16=False in TrainingArguments.\")\n",
    "    if training_arguments.bf16:\n",
    "        print(\"Warning: BF16 is not supported, but bf16=True in TrainingArguments. Setting bf16=False and fp16=True.\")\n",
    "        training_arguments.bf16 = False\n",
    "        training_arguments.fp16 = True # Fallback to fp16 if bf16 not available\n",
    "\n",
    "# Re-initialize trainer if arguments changed (e.g., bf16 status)\n",
    "# This might not be strictly necessary as args are references, but safer\n",
    "trainer.args = training_arguments\n",
    "print(\"Trainer arguments updated based on hardware support.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Start Fine-tuning\n",
    "\n",
    "Launch the training process. This will take some time depending on the dataset size, `max_steps`, and the Colab GPU assigned (T4 is slower than A100). Aiming for < 2 hours on a T4 as requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting fine-tuning...\")\n",
    "train_result = trainer.train()\n",
    "print(\"Fine-tuning finished.\")\n",
    "\n",
    "# --- Log Training Metrics ---\n",
    "metrics = train_result.metrics\n",
    "metrics[\"train_samples\"] = len(train_dataset)\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "print(\"Training metrics saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Adapter Locally\n",
    "\n",
    "Save the trained QLoRA adapter weights to the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Saving LoRA adapter to {output_dir}...\")\n",
    "trainer.save_model(output_dir) # Saves the adapter config and weights\n",
    "print(f\"Adapter saved locally to {output_dir}\")\n",
    "\n",
    "# Optional: Save the tokenizer as well (good practice)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Tokenizer saved locally to {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. (Optional) Push Adapter to Hugging Face Hub\n",
    "\n",
    "Push the trained adapter and tokenizer to your Hugging Face Hub repository for easy sharing and loading later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure hf_hub_repo_id is set correctly\n",
    "push_to_hub = True # Set to False if you don't want to push\n",
    "\n",
    "if push_to_hub:\n",
    "    print(f\"Pushing adapter and tokenizer to Hugging Face Hub repo: {hf_hub_repo_id}...\")\n",
    "    try:\n",
    "        # Push the adapter (trainer saves adapter to output_dir)\n",
    "        trainer.model.push_to_hub(hf_hub_repo_id, use_auth_token=True)\n",
    "\n",
    "        # Push the tokenizer\n",
    "        tokenizer.push_to_hub(hf_hub_repo_id, use_auth_token=True)\n",
    "\n",
    "        print(\"Successfully pushed to Hub.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error pushing to Hub: {e}\")\n",
    "else:\n",
    "    print(\"Skipping push to Hugging Face Hub.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. (Optional) Evaluation\n",
    "\n",
    "Perform evaluation on the held-out test set (if created) to calculate Perplexity and ROUGE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "if eval_dataset:\n",
    "    print(\"\\nStarting evaluation on the test set...\")\n",
    "\n",
    "    # --- Perplexity ---\n",
    "    try:\n",
    "        eval_metrics = trainer.evaluate()\n",
    "        perplexity = math.exp(eval_metrics[\"eval_loss\"])\n",
    "        print(f\"Evaluation Loss: {eval_metrics['eval_loss']:.4f}\")\n",
    "        print(f\"Perplexity: {perplexity:.4f}\")\n",
    "        # Save eval metrics\n",
    "        metrics[\"eval_perplexity\"] = perplexity\n",
    "        trainer.log_metrics(\"eval\", eval_metrics)\n",
    "        trainer.save_metrics(\"eval\", eval_metrics)\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate perplexity during evaluation: {e}\")\n",
    "\n",
    "    # --- ROUGE Score (More involved for generative tasks) ---\n",
    "    # Requires generating predictions and comparing them to references.\n",
    "    # SFTTrainer doesn't have a built-in ROUGE computation during evaluate.\n",
    "    # We need to manually generate responses for the eval set.\n",
    "\n",
    "    print(\"\\nCalculating ROUGE score (this may take a while)...\")\n",
    "    rouge_scorer = evaluate.load('rouge')\n",
    "\n",
    "    # Ensure the model is in evaluation mode and on the correct device\n",
    "    # model.eval() # Trainer usually handles this, but good practice\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    # If using the trainer's model, it should already be on the device\n",
    "    # If loading manually: model.to(device)\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Prepare inputs and get references (outputs) from the eval dataset\n",
    "    # The 'output' column in our JSONL *is* the reference/label\n",
    "    # The input to the model should be the '[INST] ... [/INST]' part\n",
    "\n",
    "    eval_batch_size = 4 # Adjust based on GPU memory\n",
    "    for i in range(0, len(eval_dataset), eval_batch_size):\n",
    "        batch_samples = eval_dataset[i:i+eval_batch_size]\n",
    "\n",
    "        # Extract the prompt part (instruction + input) for generation\n",
    "        prompts = []\n",
    "        labels = []\n",
    "        for sample in batch_samples:\n",
    "            instruction = sample['instruction']\n",
    "            context = sample['input']\n",
    "            prompt_text = f\"<s>[INST] {instruction}\\n---\\n{context}\\n--- [/INST]\" # Match training format\n",
    "            prompts.append(prompt_text)\n",
    "            labels.append(sample['output']) # The reference transcript chunk\n",
    "\n",
    "        # Tokenize prompts\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(device)\n",
    "\n",
    "        # Generate predictions\n",
    "        # Use the trainer's model directly\n",
    "        with torch.no_grad():\n",
    "             # Adjust generation parameters as needed (e.g., max_new_tokens)\n",
    "            outputs = trainer.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_seq_length, # Allow generating up to max length\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False, # Use greedy decoding for simplicity\n",
    "                # num_beams=1 # for greedy\n",
    "            )\n",
    "\n",
    "        # Decode generated sequences\n",
    "        # Important: Decode *only the generated part*, not the prompt\n",
    "        preds_decoded = []\n",
    "        for idx, output_tokens in enumerate(outputs):\n",
    "            input_token_len = len(inputs[\"input_ids\"][idx])\n",
    "            generated_tokens = output_tokens[input_token_len:]\n",
    "            pred = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "            preds_decoded.append(pred.strip())\n",
    "\n",
    "        all_preds.extend(preds_decoded)\n",
    "        all_labels.extend(labels)\n",
    "\n",
    "        if (i // eval_batch_size + 1) % 10 == 0: # Print progress every 10 batches\n",
    "            print(f\"Generated predictions for {i+eval_batch_size}/{len(eval_dataset)} samples...\")\n",
    "\n",
    "    # Compute ROUGE\n",
    "try:\n",
    "    rouge_results = rouge_scorer.compute(\n",
    "        predictions=all_preds,\n",
    "        references=all_labels\n",
    "    )\n",
    "    print(\"\\nROUGE Scores:\")\n",
    "    print(rouge_results)\n",
    "\n",
    "    # Add ROUGE-L to metrics\n",
    "    if 'rougeL' in rouge_results:\n",
    "        metrics[\"eval_rougeL\"] = rouge_results['rougeL']\n",
    "        # Log and save updated metrics\n",
    "        trainer.log_metrics(\"eval\", {\"rougeL\": rouge_results['rougeL']})\n",
    "        trainer.save_metrics(\"eval\", metrics) # Save combined eval metrics\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not compute ROUGE scores: {e}\")\n",
    "    print(\"Example Prediction:\", all_preds[0] if all_preds else \"N/A\")\n",
    "    print(\"Example Label:\", all_labels[0] if all_labels else \"N/A\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nNo evaluation dataset provided. Skipping evaluation.\")\n",
    "\n",
    "print(\"\\n--- Training and Evaluation Complete ---\")\n",
    "print(f\"Adapter saved in: {output_dir}\")\n",
    "if push_to_hub and hf_hub_repo_id:\n",
    "    print(f\"Adapter pushed to: https://huggingface.co/{hf_hub_repo_id}\")\n",
    "\n",
    "# Clean up memory (important in Colab)\n",
    "# del model\n",
    "# del trainer\n",
    "# import gc\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download Adapter\n",
    "\n",
    "If you want to download the adapter directly from Colab, you can zip the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "adapter_zip_name = f\"{os.path.basename(output_dir)}\"\n",
    "# Check if the directory exists before zipping\n",
    "if os.path.isdir(output_dir):\n",
    "    print(f\"Zipping adapter directory: {output_dir} -> {adapter_zip_name}.zip\")\n",
    "    shutil.make_archive(adapter_zip_name, 'zip', output_dir)\n",
    "    print(f\"Adapter zipped to {adapter_zip_name}.zip\")\n",
    "    # You can now download this zip file from the Colab file browser\n",
    "else:\n",
    "    print(f\"Output directory {output_dir} not found. Cannot create zip file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
